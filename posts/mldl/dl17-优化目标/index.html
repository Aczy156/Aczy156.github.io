<!DOCTYPE html>
<html><head>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Deep Learning 17 优化目标 - Aczy156</title><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
	<link rel="manifest" href="/site.webmanifest">

	<meta name="viewport" content="width=device-width, initial-scale=1">

	<meta property="og:description" content="" />
	<meta name="twitter:description" content="" />
	<meta name="description" content="" />
	<meta name="description" content="" />

	<meta property="og:title" content="Deep Learning 17 优化目标 | Aczy156" />
	<meta name="twitter:title" content="Deep Learning 17 优化目标 | Aczy156" />

	<meta property="og:image" content=""/>
	<meta itemprop="name" content="Deep Learning 17 优化目标 | Aczy156" />
	<meta name="application-name" content="Deep Learning 17 优化目标 | Aczy156" />
	<meta property="og:site_name" content="" />
	<meta property="og:title" content="Deep Learning 17 优化目标" />
<meta property="og:description" content="[TOC]
优化目标 概念区分 1、整体理解：损失函数，代价函数，目标函数，熵=&gt;都是最优化问题下的的优化目标，通过设定优化目标获取最优化问题的最优解
2、设计原理：为什么要设计这样的优化目标
 用交叉熵的原因，交叉熵和KL散度的关系 用平方损失的原因，平方损失和最大似然的关系 添加正则子的原因，为什么能添加这样的正则子  回归问题 平方「Square」 平方根「Square Root」 分类问题-熵「Entropy」 与熵相关的为信息论相关内容
熵「Entropy」  信息量
事件A的自信息量，A这个事件包含了多少信息
 $s(x)=\sum_{i}P_A(x_i)log(P_A(x_i))\$
验证：
交叉熵「Cross Entropy」  分类问题-二分类，对数似然
事件AB，从A的角度看，如何描述B
 信息论公式：
$H(A,B)=-\sum_{i}P_A(x_i)log(P_B(x_i))\$
对于样本集上的数据，两种表达形式，单个样本Loss &amp; 整个样本集然后除样本数Cost
Loss =&gt; $L=yln\hat{y}&#43;(1-y)ln(1-\hat{y})$​
Cost =&gt; $C=-\frac{1}{n}\sum_{x}^{}[yln\hat{y}&#43;(1-y)ln(1-\hat{y}]\$
验证：当真实$y=1$，如果$\hat{y}$越接近1，则损失函数Loss越小；如果$\hat{y}$越接近0，则损失函数Loss越大。因而达到忘Loss损失函数变小的方向变化。（当真实$y=0$亦然，看下图左边是$y=1$的情况，右边是$y=0$的情况）
多分类交叉熵「Categorical Cross Entropy」  分类问题-多分类，对数似然
 相对熵「Relative Entropy」  衡量两个分布的不同
事件AB，从A的角度看，B有多大不同
 $D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))\$ （对于离散事件是求和，对于连续事件是求积分）
相对熵包含了交叉熵的内容，$D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))=s(A)&#43;H(A,B)\$，也就是相对熵=坐标系自己的熵&#43;交叉熵
验证：由于坐标系选取不同，对于事件A和事件B，从A的角度看，$D_{KL}(A|B)\neq D_{KL}(B|A)$
交叉熵VS相对熵/KL散度 一般情况下，都用交叉熵，不用KL散度。
首先理解分类问题的目标 =&gt;看模型学习到的分布P(model)和真实分布P(real)是否接近 =&gt;看模型学习到的分布P(model)和训练数据P(training)的分布是否接近 =&gt;因而可以用KL散度来进行计算$D_{KL}(P(training)|P(model))$ =&gt;将KL散度进行展开$D_{KL}(P(training)|P(model))=s(P(training))&#43;H(P(training),P(model))$ =&gt;由于训练数据固定，那么对应的训练数据的熵也是不变的，所以$D_{KL}$等价于交叉熵，而因为交叉熵方便计算，所以一般都使用交叉熵" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://chenranfei.site/posts/mldl/dl17-%E4%BC%98%E5%8C%96%E7%9B%AE%E6%A0%87/" />
<meta property="article:published_time" content="2020-01-29T01:05:44+08:00" />
<meta property="article:modified_time" content="2020-01-29T01:05:44+08:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Deep Learning 17 优化目标"/>
<meta name="twitter:description" content="[TOC]
优化目标 概念区分 1、整体理解：损失函数，代价函数，目标函数，熵=&gt;都是最优化问题下的的优化目标，通过设定优化目标获取最优化问题的最优解
2、设计原理：为什么要设计这样的优化目标
 用交叉熵的原因，交叉熵和KL散度的关系 用平方损失的原因，平方损失和最大似然的关系 添加正则子的原因，为什么能添加这样的正则子  回归问题 平方「Square」 平方根「Square Root」 分类问题-熵「Entropy」 与熵相关的为信息论相关内容
熵「Entropy」  信息量
事件A的自信息量，A这个事件包含了多少信息
 $s(x)=\sum_{i}P_A(x_i)log(P_A(x_i))\$
验证：
交叉熵「Cross Entropy」  分类问题-二分类，对数似然
事件AB，从A的角度看，如何描述B
 信息论公式：
$H(A,B)=-\sum_{i}P_A(x_i)log(P_B(x_i))\$
对于样本集上的数据，两种表达形式，单个样本Loss &amp; 整个样本集然后除样本数Cost
Loss =&gt; $L=yln\hat{y}&#43;(1-y)ln(1-\hat{y})$​
Cost =&gt; $C=-\frac{1}{n}\sum_{x}^{}[yln\hat{y}&#43;(1-y)ln(1-\hat{y}]\$
验证：当真实$y=1$，如果$\hat{y}$越接近1，则损失函数Loss越小；如果$\hat{y}$越接近0，则损失函数Loss越大。因而达到忘Loss损失函数变小的方向变化。（当真实$y=0$亦然，看下图左边是$y=1$的情况，右边是$y=0$的情况）
多分类交叉熵「Categorical Cross Entropy」  分类问题-多分类，对数似然
 相对熵「Relative Entropy」  衡量两个分布的不同
事件AB，从A的角度看，B有多大不同
 $D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))\$ （对于离散事件是求和，对于连续事件是求积分）
相对熵包含了交叉熵的内容，$D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))=s(A)&#43;H(A,B)\$，也就是相对熵=坐标系自己的熵&#43;交叉熵
验证：由于坐标系选取不同，对于事件A和事件B，从A的角度看，$D_{KL}(A|B)\neq D_{KL}(B|A)$
交叉熵VS相对熵/KL散度 一般情况下，都用交叉熵，不用KL散度。
首先理解分类问题的目标 =&gt;看模型学习到的分布P(model)和真实分布P(real)是否接近 =&gt;看模型学习到的分布P(model)和训练数据P(training)的分布是否接近 =&gt;因而可以用KL散度来进行计算$D_{KL}(P(training)|P(model))$ =&gt;将KL散度进行展开$D_{KL}(P(training)|P(model))=s(P(training))&#43;H(P(training),P(model))$ =&gt;由于训练数据固定，那么对应的训练数据的熵也是不变的，所以$D_{KL}$等价于交叉熵，而因为交叉熵方便计算，所以一般都使用交叉熵"/>
<script src="http://chenranfei.site/js/feather.min.js"></script>
	
	<link href="http://chenranfei.site/css/fonts.css" rel="stylesheet">
	
	<link rel="stylesheet" type="text/css" media="screen" href="http://chenranfei.site/css/main.css" />
	
	
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
        messageStyle: "none", // 載入 MathJax 檔案時不要顯示訊息
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} // 搜尋內文 $ 符號
      });
</script>
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="http://chenranfei.site/">Aczy156</a>
	</div>
	<nav>
		
	</nav>
</header>

<main>
	<aside>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#优化目标">优化目标</a>
      <ul>
        <li><a href="#概念区分">概念区分</a></li>
        <li><a href="#回归问题">回归问题</a></li>
        <li><a href="#分类问题-熵entropy">分类问题-熵「Entropy」</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </aside>
	<article>
		<div class="title">
			<h1 class="title">Deep Learning 17 优化目标</h1>
		</div>
		

		<section class="body">
			<p>[TOC]</p>
<h2 id="优化目标">优化目标</h2>
<h3 id="概念区分">概念区分</h3>
<p>1、整体理解：损失函数，代价函数，目标函数，熵=&gt;都是最优化问题下的的<strong>优化目标</strong>，通过设定优化目标获取最优化问题的最优解</p>
<p>2、设计原理：为什么要设计这样的优化目标</p>
<ul>
<li>用交叉熵的原因，交叉熵和KL散度的关系</li>
<li>用平方损失的原因，平方损失和最大似然的关系</li>
<li>添加正则子的原因，为什么能添加这样的正则子</li>
</ul>
<h3 id="回归问题">回归问题</h3>
<h4 id="平方square">平方「Square」</h4>
<h4 id="平方根square-root">平方根「Square Root」</h4>
<h3 id="分类问题-熵entropy">分类问题-熵「Entropy」</h3>
<p>与熵相关的为信息论相关内容</p>
<h4 id="熵entropy">熵「Entropy」</h4>
<blockquote>
<p>信息量</p>
<p>事件A的自信息量，A这个事件包含了多少信息</p>
</blockquote>
<p>$s(x)=\sum_{i}P_A(x_i)log(P_A(x_i))\$</p>
<p>验证：</p>
<h4 id="交叉熵cross-entropy">交叉熵「Cross Entropy」</h4>
<blockquote>
<p>分类问题-二分类，对数似然</p>
<p>事件AB，从A的角度看，如何描述B</p>
</blockquote>
<p>信息论公式：</p>
<p>$H(A,B)=-\sum_{i}P_A(x_i)log(P_B(x_i))\$</p>
<p>对于样本集上的数据，两种表达形式，单个样本Loss &amp; 整个样本集然后除样本数Cost</p>
<p>Loss =&gt; $L=yln\hat{y}+(1-y)ln(1-\hat{y})$​</p>
<p>Cost =&gt; $C=-\frac{1}{n}\sum_{x}^{}[yln\hat{y}+(1-y)ln(1-\hat{y}]\$</p>
<p>验证：当真实$y=1$，如果$\hat{y}$越接近1，则损失函数Loss越小；如果$\hat{y}$越接近0，则损失函数Loss越大。因而达到忘Loss损失函数变小的方向变化。（当真实$y=0$亦然，看下图左边是$y=1$的情况，右边是$y=0$的情况）</p>
<p><img src="https://pic1.zhimg.com/80/v2-8d5eac16bfc171869da269efdcddcc24_1440w.jpg" alt="img" style="zoom:50%;" /><img src="https://pic3.zhimg.com/80/v2-06cdd7ba5d82d78d784a52ccdcd633ae_1440w.jpg" alt="img" style="zoom:50%;" /></p>
<h4 id="多分类交叉熵categorical-cross-entropy">多分类交叉熵「Categorical Cross Entropy」</h4>
<blockquote>
<p>分类问题-多分类，对数似然</p>
</blockquote>
<h4 id="相对熵relative-entropy">相对熵「Relative Entropy」</h4>
<blockquote>
<p>衡量两个分布的不同</p>
<p>事件AB，从A的角度看，B有多大不同</p>
</blockquote>
<p>$D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))\$ （对于离散事件是求和，对于连续事件是求积分）</p>
<p>相对熵包含了交叉熵的内容，$D_{KL}(A|B)=\sum_{i}P_A(x_i)log(P_A(x_i))-\sum_{i}P_A(x_i)log(P_B(x_i))=s(A)+H(A,B)\$，也就是<code>相对熵=坐标系自己的熵+交叉熵</code></p>
<p>验证：由于坐标系选取不同，对于事件A和事件B，从A的角度看，$D_{KL}(A|B)\neq D_{KL}(B|A)$</p>
<h4 id="交叉熵vs相对熵kl散度">交叉熵VS相对熵/KL散度</h4>
<p>一般情况下，都用交叉熵，不用KL散度。</p>
<p>首先理解分类问题的目标
=&gt;看<strong>模型学习到的分布P(model)和真实分布P(real)<strong>是否接近
=&gt;看</strong>模型学习到的分布P(model)和训练数据P(training)的分布</strong>是否接近
=&gt;因而可以用KL散度来进行计算$D_{KL}(P(training)|P(model))$
=&gt;将KL散度进行展开$D_{KL}(P(training)|P(model))=s(P(training))+H(P(training),P(model))$
=&gt;由于训练数据固定，那么对应的训练数据的熵也是不变的，所以$D_{KL}$等价于交叉熵，而因为交叉熵方便计算，所以一般都使用交叉熵</p>

		</section>

		<div class="meta">Posted on Jan 29, 2020</div>

		<div class="post-tags">
			
			
			<nav class="nav tags">
				<ul class="tags">
					
					<li><a href="/tags/dl">DL</a></li>
					
					<li><a href="/tags/%E7%AE%97%E6%B3%95">算法</a></li>
					
				</ul>
			</nav>
			
			
		</div>
	</article>
	
</main>
<footer>
<hr><a class="soc" href="https://github.com/Aczy156" title="GitHub"><i data-feather="github"></i></a>|⚡️
	2022  © Aczy156 |  <a href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
</footer>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-168772474-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>

<script>
      feather.replace()
</script></div>
    </body>
</html>
